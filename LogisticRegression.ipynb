import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV
import joblib
Process Data
Data Cleanup
In [2]:
data = pd.read_csv("../Resources/exoplanet_data.csv")

# Drop null columns
data = data.dropna(axis='columns', how='all')

# Drop null rows
data = data.dropna()

# Convert dtypes of int64 to float64
for column, content in data.items():
    if data[column].dtype == 'int64':
        data = data.astype({column: 'float64'})
Pre-prossessing
In [3]:
# Assign data to X and y
X = data.drop("koi_disposition", axis=1)
y = data["koi_disposition"]

# Split data into training and testing groups
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)
In [4]:
# Scale X values
X_scaler = MinMaxScaler().fit(X_train)
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)
Build the Model
Train the Model
In [5]:
model_1 = LogisticRegression(solver='newton-cg', multi_class='auto')
model_1.fit(X_train_scaled, y_train)

model_1_training_score = round(model_1.score(X_train_scaled, y_train)*100,3)
base_accuracy = round(model_1.score(X_test_scaled, y_test)*100,3)

print(f"Training Data Score: {model_1_training_score} %")
print(f"Testing Data Score: {base_accuracy} %")
Training Data Score: 85.504 %
Testing Data Score: 86.213 %
Select Features
In [6]:
# Evaluate features
feature_names = X.columns.tolist()
selector = RFECV(estimator=model_1, cv=5, step=1)
_ = selector.fit(X_train_scaled, y_train)

# Determine which features ought to be kept
preSelected_features = sorted(zip(selector.ranking_, feature_names))
ranked_features = pd.DataFrame(preSelected_features, columns=['Ranking', 'Feature'])
ranked_features = ranked_features.set_index('Feature')
ranked_features
Out[7]:
Ranking
Feature	
dec	1
koi_depth	1
koi_duration	1
koi_duration_err1	1
koi_duration_err2	1
koi_fpflag_co	1
koi_fpflag_ec	1
koi_fpflag_nt	1
koi_fpflag_ss	1
koi_impact	1
koi_impact_err1	1
koi_impact_err2	1
koi_model_snr	1
koi_period	1
koi_period_err1	1
koi_period_err2	1
koi_slogg	1
koi_slogg_err1	1
koi_slogg_err2	1
koi_srad	1
koi_srad_err1	1
koi_srad_err2	1
koi_steff	1
koi_steff_err1	1
koi_steff_err2	1
koi_tce_plnt_num	1
koi_teq	1
koi_time0bk	1
koi_time0bk_err1	1
koi_time0bk_err2	1
ra	1
koi_kepmag	2
koi_insol_err1	3
koi_prad_err1	4
koi_prad	5
koi_insol	6
koi_insol_err2	7
koi_prad_err2	8
koi_depth_err2	9
koi_depth_err1	10
# Remove features with Ranking > 16
selected_features = []
for tup in preSelected_features:
    if tup[0] < 17:
        selected_features.append(tup[1])
In [9]:
# Use new data for all subsequent models
## Assign new data to X 
X_train_select = X_train[selected_features]
X_test_select = X_test[selected_features]

X_scaler = MinMaxScaler().fit(X_train_select)
X_train_scaled = X_scaler.transform(X_train_select)
X_test_scaled = X_scaler.transform(X_test_select)

## Train new model
model_2 = LogisticRegression(solver='newton-cg', multi_class='auto')
model_2.fit(X_train_scaled, y_train)

model_2_training_score = round(model_2.score(X_train_scaled, y_train)*100,3)
select_features_accuracy = round(model_2.score(X_test_scaled, y_test)*100,3)

print(f"Training Data Score: {model_2_training_score} %")
print(f"Testing Data Score: {select_features_accuracy} %")
Training Data Score: 85.504 %
Testing Data Score: 86.213 %

Model Tuning
In [12]:
# Create the GridSearchCV model
model_3 = LogisticRegression(solver='newton-cg', multi_class='auto')

param_grid = {
    'C': np.logspace(0, 4, 10),
    'penalty': ['l2']
}
grid = GridSearchCV(model_3, param_grid, cv=5, verbose=0)

# Train the model with GridSearch
_ = grid.fit(X_train_scaled, y_train)
Train Tuned Model
In [13]:
# Tuned parameters
C = grid.best_params_['C']
penalty = grid.best_params_['penalty']

# Tuned model
tuned_model = LogisticRegression(solver='newton-cg', multi_class='auto',
                                 C=C, penalty=penalty)
tuned_model.fit(X_train_scaled, y_train)

model_3_training_score = round(tuned_model.score(X_train_scaled, y_train)*100,3)
tuned_accuracy = round(tuned_model.score(X_test_scaled, y_test)*100,3)

print(f"Training Data Score: {model_3_training_score} %")
print(f"Testing Data Score: {tuned_accuracy} %")
Training Data Score: 88.709 %
Testing Data Score: 89.474 %
Model Predictions and Evaluations
Predictions
In [14]:
predictions = tuned_model.predict(X_test_scaled)
classifications = y_test.unique().tolist()

prediction_actual = {
    'Actual': y_test,
    'Prediction': predictions
}

PA_df = pd.DataFrame(prediction_actual)
PA_df = PA_df.set_index('Actual').reset_index()
PA_df.head(15)
Out[14]:
Actual	Prediction
0	CANDIDATE	CANDIDATE
1	FALSE POSITIVE	FALSE POSITIVE
2	FALSE POSITIVE	FALSE POSITIVE
3	FALSE POSITIVE	FALSE POSITIVE
4	CANDIDATE	CANDIDATE
5	FALSE POSITIVE	FALSE POSITIVE
6	CANDIDATE	CANDIDATE
7	FALSE POSITIVE	FALSE POSITIVE
8	FALSE POSITIVE	FALSE POSITIVE
9	FALSE POSITIVE	FALSE POSITIVE
10	FALSE POSITIVE	FALSE POSITIVE
11	FALSE POSITIVE	FALSE POSITIVE
12	CONFIRMED	CONFIRMED
13	FALSE POSITIVE	FALSE POSITIVE
14	FALSE POSITIVE	FALSE POSITIVE

Evaluations
In [15]:
evaluations = {'': ['Base Model', 'Select Features Model', 'Tuned Model'],
               'Accuracy': [f"{base_accuracy}%", f"{select_features_accuracy}%", f"{tuned_accuracy}%"]}

evaluations_df = pd.DataFrame(evaluations)
evaluations_df = evaluations_df.set_index('')

evaluations_df.to_csv('../Resources/LogisticRegression_eval.csv')
evaluations_df
Out[15]:
Accuracy
Base Model	86.213%
Select Features Model	86.213%
Tuned Model	89.474%
Save Model
In [16]:
filename = '../Models/OtherModel_LogisticRegression.sav'
_ = joblib.dump(tuned_model, filename)

